{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db5145a4",
   "metadata": {},
   "source": [
    "# Lecture 7: Tokenization Deep Dive\n",
    "Welcome to Lecture 7 of AI Engineering Essentials (Part 2)! In this notebook, we'll break down how text is transformed into numbers for machine learning models. You'll see how tokenization works, what Byte Pair Encoding (BPE) is, how attention masks help, and even try out some multilingual examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a640030",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "- Understand how BPE tokenization works\n",
    "- Explore attention masks and their purpose\n",
    "- Tokenize a French sentence and compare models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d26f7f1",
   "metadata": {},
   "source": [
    "## 1. Setup: Import Libraries and Load a Tokenizer\n",
    "Let's start by importing the Hugging Face Transformers library and loading a basic tokenizer. We'll use `bert-base-uncased` for our first examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a53b8",
   "metadata": {},
   "source": [
    "## 2. Tokenize a Simple English Sentence\n",
    "Let's see how a basic English sentence gets split into tokens and IDs. This is the first step in turning text into something a model can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d6f8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['token', '##ization', 'is', 'the', 'first', 'step', 'in', 'nl', '##p', 'magic', '!']\n",
      "Token IDs: [19204, 3989, 2003, 1996, 2034, 3357, 1999, 17953, 2361, 3894, 999]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Tokenization is the first step in NLP magic!\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdaa0c4",
   "metadata": {},
   "source": [
    "## 3. Byte Pair Encoding (BPE) Explained\n",
    "Byte Pair Encoding (BPE) is a clever way to break words into smaller pieces (subwords). This helps the model handle new or rare words by splitting them into known chunks. Let's see how BPE works in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bffb5ce",
   "metadata": {},
   "source": [
    "**How BPE works:**\n",
    "1. Start with all characters as your vocabulary.\n",
    "2. Find the most common pair of symbols (characters or subwords).\n",
    "3. Merge them into a new symbol.\n",
    "4. Repeat until you reach a set number of merges or can't merge anymore.\n",
    "\n",
    "This keeps the vocabulary small and lets the model handle words it's never seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fff7c6",
   "metadata": {},
   "source": [
    "## 4. Visualizing Token Splits for a Complex Word\n",
    "Let's pick a tricky word and see how the tokenizer breaks it down. We'll use \"unbelievable\" as our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e8bc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['unbelievable']\n",
      "Token IDs: [23653]\n"
     ]
    }
   ],
   "source": [
    "word = \"unbelievable\"\n",
    "tokens = tokenizer.tokenize(word)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6c8484",
   "metadata": {},
   "source": [
    "Notice how the word is split into smaller pieces. This is BPE in action! It helps the model understand and process words it might not have seen during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37777bbc",
   "metadata": {},
   "source": [
    "## 5. Attention Masks: What and Why?\n",
    "When working with batches of sentences, we often need to pad them so they're all the same length. Attention masks tell the model which parts are real data and which are just padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d51df5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs:\n",
      " tensor([[  101, 19204,  3989,  2003,  4569,   999,   102,     0,     0,     0,\n",
      "             0],\n",
      "        [  101,  2292,  1005,  1055,  2156,  2129,  3086, 15806,  2147,  1012,\n",
      "           102]])\n",
      "Attention Masks:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"Tokenization is fun!\",\n",
    "    \"Let's see how attention masks work.\"\n",
    "]\n",
    "\n",
    "encoded = tokenizer(\n",
    "    sentences,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"Input IDs:\\n\", encoded[\"input_ids\"])\n",
    "print(\"Attention Masks:\\n\", encoded[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450d5558",
   "metadata": {},
   "source": [
    "The attention mask is a series of 1s and 0s. 1 means \"real token,\" 0 means \"padding.\" This helps the model ignore the padding during training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbaf4a3",
   "metadata": {},
   "source": [
    "## 6. Multilingual Example: Tokenizing French\n",
    "Let's see how the tokenizer handles a French sentence. We'll also compare how different models split the same sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc7963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT tokens: ['l', \"'\", 'intelligence', 'art', '##ific', '##iel', '##le', 'change', 'le', 'monde', '.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5634c812dd74d2e9593b6e2d17c2e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shahe\\Documents\\SuperDataScience\\Official Work\\Courses\\ai-engineering\\ai_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shahe\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66492f4058b04bbfb440353d56c1b7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb1b8b545304bef9b922129b5906a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963a4af06b97431692666946700e35f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilingual BERT tokens: ['L', \"'\", 'intelligence', 'arti', '##ficie', '##lle', 'change', 'le', 'monde', '.']\n"
     ]
    }
   ],
   "source": [
    "french_sentence = \"L'intelligence artificielle change le monde.\"\n",
    "\n",
    "french_tokens_bert = tokenizer.tokenize(french_sentence)\n",
    "print(\"BERT tokens:\", french_tokens_bert)\n",
    "\n",
    "multi_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "french_tokens_multi = multi_tokenizer.tokenize(french_sentence)\n",
    "print(\"Multilingual BERT tokens:\", french_tokens_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7517a6",
   "metadata": {},
   "source": [
    "Notice the difference in how the two tokenizers split the French sentence. Multilingual models are better at handling non-English text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e09d0b8",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "- Tokenization is the first step in NLP magicâ€”understanding it helps you debug and optimize your pipelines.\n",
    "- BPE lets models handle new words by breaking them into subwords.\n",
    "- Attention masks help models ignore padding.\n",
    "- Multilingual tokenizers are great for non-English text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517299e2",
   "metadata": {},
   "source": [
    "## Call to Action\n",
    "Try tokenizing a sentence in your native language and share your findings in the course repo!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
